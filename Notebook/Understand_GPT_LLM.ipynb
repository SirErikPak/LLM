{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc49dac",
   "metadata": {},
   "source": [
    "# Prepare Dataset from the LLM Project\n",
    "- The concept of converting data into a vector format is often referred to as embedding. Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text, as illustrated in figure 2.2. However, it’s important to note that different data formats require distinct embedding models. For example, an embedding model designed for text would not be suitable for embedding audio or video data.\n",
    "- At its core, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space—the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.\n",
    "- While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for retrieval-augmented generation. Retrievalaugmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text, which is a technique that is beyond the scope of this book. Since our goal is to train GPT-like LLMs, which learn to generate text one word at a time, we will focus on word embeddings.\n",
    "- Several algorithms and frameworks have been developed to generate word embeddings. One of the earlier and most popular examples is the Word2Vec approach. Word2Vec trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa. The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar  meanings. Consequently, when projected into two-dimensional word embeddings for visualization purposes, similar terms are clustered together.\n",
    "- Word embeddings can have varying dimensions, from one to thousands. A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency.\n",
    "- While we can use pretrained models such as Word2Vec to generate embeddings for machine learning models, LLMs commonly produce their own embeddings that are part of the input layer and are updated during training. The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand. (LLMs can also create contextualized output embeddings.)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0deca49",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing Text\n",
    "- Let’s discuss how we split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM. These tokens are either individual words or special characters, including punctuation characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61332589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: ../Data/verdict.txt. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# efine the file_path\n",
    "file_path = \"../Data/verdict.txt\"\n",
    "\n",
    "# get data from a URL\n",
    "if not os.path.exists(\"../Data/verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"Data downloaded and saved to:\", file_path)\n",
    "else:\n",
    "    print(f\"File already exists: {file_path}. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938cedc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# read the content of the file (Reading in a short story as text sample into Python)\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77368ce",
   "metadata": {},
   "source": [
    "Our goal is to tokenize this 20,479-character short story into individual words and special characters that we can then turn into embeddings for LLM training.\n",
    "\n",
    "**NOTE** It’s common to process millions of articles and hundreds of thousands of books—many gigabytes of text—when working with LLMs. However, for educational purposes, it’s sufficient to work with smaller text samples like a single book to illustrate the main ideas behind the text processing steps and to make it possible to run it in a reasonable time on consumer hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d0fe9",
   "metadata": {},
   "source": [
    "Using some simple example text, we can use the re.split command with the following syntax to split a text on whitespace characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5640d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90a516",
   "metadata": {},
   "source": [
    "The result is a list of individual words, whitespaces, and punctuation characters:\n",
    "\n",
    "This simple tokenization scheme mostly works for separating the example text into individual words; however, some words are still connected to punctuation characters that we want to have as separate list entries. We also refrain from making all text lowercase because capitalization helps LLMs distinguish between proper nouns and common nouns, understand sentence structure, and learn to generate text with proper\n",
    "capitalization.\n",
    "\n",
    "Let’s modify the regular expression splits on whitespaces (\\s), commas, and periods ([,.]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4297df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.!]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8230823",
   "metadata": {},
   "source": [
    "We can see that the words and punctuation characters are now separate list entries just as we wanted:\n",
    "A small remaining problem is that the list still includes whitespace characters. Optionally, we can remove these redundant characters safely as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe20cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# strip white space\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456a168",
   "metadata": {},
   "source": [
    "**NOTE** When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eab11d",
   "metadata": {},
   "source": [
    "The tokenization scheme we devised here works well on the simple sample text. Let’s modify it a bit further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of Edith Wharton’s short story, along with additional special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8613806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\'\\s]|--)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503cab7d",
   "metadata": {},
   "source": [
    "## Break Down Text into Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66db4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.:;?_!\"()\\'\\s]|--)', raw_text)\n",
    "result = [item for item in result if item.strip()]\n",
    "preprocessed = result\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf7269",
   "metadata": {},
   "source": [
    "## 2.3 Converting tokens into tokin IDs\n",
    "- Next, let’s convert these tokens from a Python string to an integer representation to produce the token IDs. This conversion is an intermediate step before converting the token IDs into embedding vectors. To map the previously generated tokens into token IDs, we have to build a vocabulary first. This vocabulary defines how we map each unique word and special character to a unique integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36904a42",
   "metadata": {},
   "source": [
    "We build a vocabulary by tokenizing the entire text in a training dataset into individual tokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed. The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique token to a unique integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a9781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocabulary_size = len(all_words)\n",
    "print(\"Vocabulary size:\", len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a7605ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from tokens to unique integer IDs (Creating a vocabulary)\n",
    "# This is a simple vocabulary mapping where each unique token is assigned a unique integer ID.\n",
    "# In practice, this mapping is often more complex and may include special tokens for padding, unknown words, etc.\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefe3fe",
   "metadata": {},
   "source": [
    "When we want to convert the outputs of an LLM from numbers back into text, we need a way to turn token IDs into text. For this, we can create an inverse version of the vocabulary that maps token IDs back to the corresponding text tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c574a2",
   "metadata": {},
   "source": [
    "Let’s implement a complete tokenizer class in Python with an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. In addition, we’ll implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text. The following listing shows the code for this tokenizer implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "239f2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Split the text into tokens based on punctuation and whitespace\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'\\s]|--)', text)\n",
    "        # Remove empty strings and strip whitespace\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        # Convert tokens to their corresponding integer IDs using the vocabulary\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, idx):\n",
    "        # Convert integer IDs back to tokens using the inverse vocabulary\n",
    "        text = \" \".join([self.int_to_str[i] for i in idx])\n",
    "        # replace spaces before the specific punctuation\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c593812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a new tokenizer object from the SimpleTokenizerV1 class\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "            Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(decoded)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca13d73",
   "metadata": {},
   "source": [
    "Tokenizer implementations share two common methods: an encode method and a decode method. The encode method takes in the sample text, splits it into individual tokens, and converts the tokens into token IDs via the vocabulary. The decode method takes in token IDs, converts them back into text tokens, and concatenates the text tokens into natural text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ed68b",
   "metadata": {},
   "source": [
    "## 2.4 Adding special context tokens\n",
    "- We need to modify the tokenizer to handle unknown words. We also need to address the usage and addition of special context tokens that can enhance a model’s understanding of context or other relevant information in the text. These special tokens can include markers for unknown words and document boundaries, for example. In particular, we will modify the vocabulary and tokenizer, SimpleTokenizerV2, to support two new tokens, <|unk|> and <|endoftext|>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5287555a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.encode(text))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     10\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Convert tokens to their corresponding integer IDs using the vocabulary\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28mself\u001b[39m.str_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     10\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Convert tokens to their corresponding integer IDs using the vocabulary\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28mself\u001b[39m.str_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be254d47",
   "metadata": {},
   "source": [
    "Next, we will test the tokenizer further on text that contains unknown words and discuss additional special tokens that can be used to provide further context for an LLM during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79900896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1132\n"
     ]
    }
   ],
   "source": [
    "# add end of text and unknown tokens\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext|>','<|unk|>']) # place holder tokens\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aed52852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[:5]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60cddb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b517d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'\\s]|--)', text)\n",
    "        # strip white space\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        # replace unknown tokens with <|unk|>\n",
    "        preprocessed = [item if item in self.str_to_int \n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    "        # convert to indices\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, idx):\n",
    "        text = \" \".join([self.int_to_str[i] for i in idx])\n",
    "        # replace spaces before the specific punctuation\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8d42848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a93179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d3ece54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8b23c",
   "metadata": {},
   "source": [
    "Depending on the LLM, some researchers also consider additional special tokens such as the following:\n",
    "- [BOS] (beginning of sequence)—This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
    "- [EOS] (end of sequence)—This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins.\n",
    "- [PAD] (padding)—When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch.\n",
    "- The tokenizer used for GPT models does not need any of these tokens; it only uses an <|endoftext|> token for simplicity. <|endoftext|> is analogous to the [EOS] token. <|endoftext|> is also used for padding. However, as we’ll explore in subsequent chapters, when training on batched inputs, we typically use a mask, meaning we don’t attend to padded tokens. Thus, the specific token chosen for padding becomes inconsequential. Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token for out-of-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks words down into subword units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65086b9f",
   "metadata": {},
   "source": [
    "## 2.5 Byte pair encoding (BPE)\n",
    "- Let’s look at a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE). The BPE tokenizer was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT. Since implementing BPE can be relatively complicated, we will use an existing Python open source library called *tiktoken* (https://github.com/openai/tiktoken), which implements the BPE algorithm very efficiently based on source code in Rust.\n",
    "- Byte Pair Encoding (BPE) is a data compression and tokenization algorithm originally developed for text compression but now widely used in natural language processing (NLP) for subword tokenization. The main idea behind BPE is to iteratively merge the most frequent pairs of consecutive bytes (characters or subwords) in a corpus to form new tokens until a desired vocabulary size is reached.\n",
    "    - Here’s how BPE works in NLP, step-by-step:\n",
    "        - Start with a Character Vocabulary: Initialize the vocabulary with all unique characters in the text.\n",
    "        - Find the Most Frequent Pair: Scan the entire corpus and identify the most frequent pair of adjacent characters or tokens.\n",
    "        - Merge the Pair: Replace every occurrence of the most frequent pair with a new combined token (subword), effectively reducing the total number of tokens.\n",
    "        - Repeat: Continue this process of finding and merging the most frequent pairs until the vocabulary reaches a predefined size or no more pairs meet the frequency threshold.\n",
    "        - This process turns characters into subwords and eventually into word-level tokens, allowing the model to represent rare or unseen words as combinations of known subwords.\n",
    "    - Advantages of BPE include:\n",
    "        - Handling Out-of-Vocabulary (OOV) Words: New or rare words can be decomposed into familiar subwords, avoiding the problem of unknown tokens.\n",
    "        - Effective for Morphologically Rich Languages: BPE captures common prefixes, suffixes, and roots, helping in languages with complex word formation.\n",
    "        - Balancing Vocabulary Size: BPE strikes a balance between large word-level vocabulary (which can be huge) and small character-level vocabulary (which can be inefficient).\n",
    "- For example, if the word \"tokenizer\" is not in the vocabulary, it might be split into \"token\" + \"izer,\" both known subwords, allowing the model to understand and process it effectively.\n",
    "- In summary, BPE iteratively merges frequent pairs to build a flexible vocabulary of subwords that models can use efficiently, enabling them to handle diverse and complex linguistic phenomena while controlling vocabulary size and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "938ca004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d14d3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2821a70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496,\n",
       " 11,\n",
       " 466,\n",
       " 345,\n",
       " 588,\n",
       " 8887,\n",
       " 30,\n",
       " 220,\n",
       " 50256,\n",
       " 554,\n",
       " 262,\n",
       " 4252,\n",
       " 18250,\n",
       " 8812,\n",
       " 2114,\n",
       " 1659,\n",
       " 617,\n",
       " 34680,\n",
       " 27271,\n",
       " 13]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "# Use allowed_special to include <|endoftext|> token\n",
    "tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a63fb94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text, allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5c60a",
   "metadata": {},
   "source": [
    "We can make two noteworthy observations based on the token IDs and decoded text. First, the <|endoftext|> token is assigned a relatively large token ID, namely, 50,256. In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID. Second, the BPE tokenizer encodes and decodes unknown words, such as someunknownPlace, correctly. The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens? The algorithm underlying BPE breaks down words that aren’t in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words. So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135af1fb",
   "metadata": {},
   "source": [
    "## 2.6 Data sampling with a sliding window\n",
    "- Given a text sample, extract input blocks as subsamples that serve as input to the LLM, and the LLM’s prediction task during training is to predict the next word that follows the input block. During training, we mask out all words that are past the target. Note that the text shown in this figure must undergo tokenization before the LLM can process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c20ce6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# Read the raw text from the vocabulary file\n",
    "with open(\"../Data/verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bbff61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 50 tokens as a sample\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba18dc3",
   "metadata": {},
   "source": [
    "One of the easiest and most intuitive ways to create the input–target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61906019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# The context size determines how many tokens are included in the input. (chunk)\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "073ffab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "# By processing the inputs along with the targets, which are the inputs shifted by one \n",
    "# position, we can create the next-word prediction tasks\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e55d9e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa03e5",
   "metadata": {},
   "source": [
    "There’s only one more task before we can turn the tokens into embeddings: implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays. In particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d2125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "        # Use sliding window to chunk the book in to overlapping sequence of text length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            # Ensure the chunks are of the same length\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                      stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ba6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ea96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    "    )\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a757b",
   "metadata": {},
   "source": [
    "## Creating token embeddings\n",
    "- The last step in preparing the input text for LLM training is to convert the token IDs\n",
    "into embedding vectors,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69349f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43627110",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_idx = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# Create token embeddings\n",
    "# The last step in preparing the input text for LLM training is to convert the token IDs\n",
    "# into embedding vectors, which are trainable parameters in the model.\n",
    "# This is typically done using an embedding layer in PyTorch.\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e29cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e10aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(torch.tensor([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(torch.tensor([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79769e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(input_idx.detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb7263",
   "metadata": {},
   "source": [
    "## Encoding word positions\n",
    "- In principle, token embeddings are a suitable input for an LLM. However, a minor shortcoming of LLMs is that their self-attention mechanism doesn’t have a notion of position or order for the tokens within a sequence. The way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, \n",
    "                                  max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"\\nToken embeddings shape:\\n\", token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32addf",
   "metadata": {},
   "source": [
    "The 8 × 4 × 256–dimensional tensor output shows that each token ID is now embedded\n",
    "as a 256-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcacc6a",
   "metadata": {},
   "source": [
    "For a GPT model’s absolute embedding approach, we just need to create another\n",
    "embedding layer that has the same embedding dimension as the token_embedding_\n",
    "layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length, torch.arange(context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c849618",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea03c1",
   "metadata": {},
   "source": [
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
    "We can now add these directly to the token embeddings, where PyTorch will add\n",
    "the 4 × 256–dimensional pos_embeddings tensor to each 4 × 256–dimensional token\n",
    "embedding tensor in each of the eight batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding_layer(torch.arange(context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine token embeddings and positional embeddings\n",
    "# The final input to the LLM is the sum of the token embeddings and the positional embeddings.\n",
    "# This way, the model can learn to associate each token with its position in the sequence.\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"\\nInput embeddings shape:\\n\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b4082",
   "metadata": {},
   "source": [
    "## Coding attention mechanisms\n",
    "### The \"self\" in self-attention\n",
    "In self-attention, the \"self\" refers to the mechanism’s ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image. This is in contrast to traditional attention mechanisms, where the focus is on the relationships between elements of two different sequences, such as in sequence-to sequence models where the attention might be between an input sequence and an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5556140",
   "metadata": {},
   "source": [
    "## A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts (x^3)\n",
    "     [0.22, 0.58, 0.33], # with (x^4)\n",
    "     [0.77, 0.25, 0.10], # one (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4697833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input query is the second item in the inputs tensor\n",
    "input_query = inputs[1]\n",
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513cc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = inputs[0]\n",
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5500 * 0.4300 + 0.8700 * 0.1500 + 0.6600 * 0.8900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c52bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dot(input_1, input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c660111",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, ele in enumerate(inputs[0]):\n",
    "    # print(ele)\n",
    "    print(inputs[0][idx] * input_query[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88738088",
   "metadata": {},
   "source": [
    "illustrates how we calculate the intermediate attention scores between the query token and each input token. We determine these scores by computing the dot product of the query, x(2), with every other input token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de34f8",
   "metadata": {},
   "source": [
    "## Understanding dot products\n",
    "A dot product is essentially a concise way of multiplying two vectors element-wise and\n",
    "then summing the products, which can be demonstrated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0.\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e19c3",
   "metadata": {},
   "source": [
    "Beyond viewing the dot product operation as a mathematical tool that combines two vectors to yield a scalar value, the dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot product indicates a greater degree of alignment or similarity between the vectors. In the context of self-attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or “attends to,” any other element: the higher the dot product, the higher the similarity and attention score between two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374efd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the attention scores\n",
    "attn_scores_2_temp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "attn_scores_2_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores_2_temp.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67577fbd",
   "metadata": {},
   "source": [
    "In practice, it’s more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. The following is a basic implementation of the softmax function for normalizing the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not stable for certain values, e.g., large values\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e0aaf",
   "metadata": {},
   "source": [
    "In addition, the softmax function ensures that the attention weights are always positive. This makes the output interpretable as probabilities or relative importance, where higher weights indicate greater importance. Note that this naive softmax implementation (softmax_naive) may encounter numerical instability problems, such as overflow and underflow, when dealing with large or small input values. Therefore, in practice, it’s advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to use optimized softmax function\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1] # Use the second input as the query\n",
    "\n",
    "contex_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    contex_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(\"Context vector:\", contex_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06483b20",
   "metadata": {},
   "source": [
    "## A simple self-attention mechanism without trainable weights\n",
    "- Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6596cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts (x^3)\n",
    "     [0.22, 0.58, 0.33], # with (x^4)\n",
    "     [0.77, 0.25, 0.10], # one (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9179c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a331cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using matrix multiplication to compute attention scores\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953eeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights\n",
    "print(attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using matrix multiplication to compute attention scores\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aeed63",
   "metadata": {},
   "source": [
    "## Implementing self-attention with trainable weights\n",
    "Our next step will be to implement the self-attention mechanism used in the original transformer architecture, the GPT models, and most other popular LLMs. This self-attention mechanism is also called scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "print(\"W_query:\", W_query)\n",
    "print(\"W_key:\", W_key)\n",
    "print(\"W_value:\", W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "query_2, key_2, value_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc20249",
   "metadata": {},
   "source": [
    "The output for the query results in a two-dimensional vector since we set the number of columns of the corresponding weight matrix, via d_out, to 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6acf4a5",
   "metadata": {},
   "source": [
    "## Weight parameters vs. attention weights\n",
    "- In the weight matrices W, the term “weight” is short for “weight parameters,” the values of a neural network that are optimized during training. This is not to be confused with the attention weights. As we already saw, attention weights determine the extent to which a context vector depends on the different parts of the input (i.e., to what\n",
    "extent the network focuses on different parts of the input). In summary, weight parameters are the fundamental, learned coefficients that define the network’s connections, while attention weights are dynamic, context-specific values.\n",
    "- In essence, matrix multiplication encapsulates the chaining of linear geometric operations: every time you compose two or more geometric linear transformations, the matrix product gives you a single matrix representing their combined effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89afa35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c411fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0461702",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b4e477",
   "metadata": {},
   "source": [
    "Now, we want to go from the attention scores to the attention weights. We compute the attention weights by scaling the attention scores and using the softmax function. However, now we scale the attention scores by dividing\n",
    "them by the square root of the embedding dimension of the keys (taking the square root is mathematically the same as exponentiating by 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e07262",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f3478",
   "metadata": {},
   "source": [
    "#### The rationale behind scaled-dot product attention:\n",
    "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate. The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cdd94",
   "metadata": {},
   "source": [
    "Similar to when we computed the context vector as a weighted sum over the input vectors, we now compute the context vector as a weighted sum over the value vectors. Here, the attention weights serve as a weighting factor that weighsthe respective importance of each value vector. Also as before, we can use matrix multiplication to obtain the output in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d764d9d",
   "metadata": {},
   "source": [
    "### Why query, key, and value?\n",
    "The terms “key,” “query,” and “value” in the context of attention mechanisms are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search, and retrieve information. A query is analogous to a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them. The key is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match the query. The value in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89afef4",
   "metadata": {},
   "source": [
    "## 3.4.2 Implementing a compact self-attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46382609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute queries, keys, and values vectors\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute queries, keys, and values vectors\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(789)\n",
    "sa_v1 = SelfAttention_v2(d_in, d_out)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56756da2",
   "metadata": {},
   "source": [
    "#### Comparing SelfAttention_v1 and SelfAttention_v2\n",
    "Note that nn.Linear in SelfAttention_v2 uses a different weight initialization scheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1, which causes both mechanisms to produce different results. To check that both implementations, SelfAttention_v1 and SelfAttention_v2, are otherwise similar, we can transfer the weight matrices from a SelfAttention_v2 object to a Self-Attention_v1, such that both objects then produce the same results. Your task is to correctly assign the weights from an instance of SelfAttention_v2 to an instance of SelfAttention_v1. To do this, you need to understand the relationship between the weights in both versions. (Hint: nn.Linear stores the weight matrix in a transposed form.) After the assignment, you should observe that both instances produce the same outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45f534",
   "metadata": {},
   "source": [
    "## 3.5 Hiding future words with causal attention\n",
    "### 3.5.1 Apply a causal attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7db8c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
